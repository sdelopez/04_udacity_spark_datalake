{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format, dayofweek, to_timestamp\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get config information for Local/Cloud development\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Credentials for AWS Cloud development\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config.get('AWS','AWS_ACCESS_KEY_ID')\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config.get('AWS','AWS_SECRET_ACCESS_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get S3 path when working on AWS - Dont use for local developement\n",
    "\n",
    "input_data = config.get('S3','INPUT_DATA')\n",
    "output_data = config.get('S3','OUTPUT_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get local path for Input/output data - Don't use for AWS developement\n",
    "\n",
    "input_data = config.get('LOCAL','INPUT_DATA') \n",
    "output_data = config.get('LOCAL','OUTPUT_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function o create a spark session\n",
    "\n",
    "def create_spark_session():\n",
    "    '''\n",
    "    Initializes global spark session object with relevant packages and AWS configs\n",
    "    '''\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create spark session\n",
    "spark = create_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Schema for Song Play Analysis\n",
    "\n",
    "### Dataset - Raw JSON data structures\n",
    "\n",
    "* **log_data**: log_data contains data about what users have done (columns: event_id, artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId)\n",
    "* **song_data**: song_data contains data about songs and artists (columns: num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year)\n",
    "\n",
    "\n",
    "Using the song and log datasets, we will create a star schema optimized for queries on song play analysis. This includes the following tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact Table\n",
    "\n",
    "* **songplays**: song play data together with user, artist, and song info (songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "\n",
    "### Dimension Tables\n",
    "\n",
    "* **users**: user info (columns: user_id, first_name, last_name, gender, level)\n",
    "* **songs**: song info (columns: song_id, title, artist_id, year, duration)\n",
    "* **artists**: artist info (columns: artist_id, name, location, latitude, longitude)\n",
    "* **time**: detailed time info about song plays (columns: start_time, hour, day, week, month, year, weekday)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Process Song Data : Define **songs_table** and **artists_table** from **songs_data**\n",
    "## load song_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to song data file\n",
    "song_data = input_data + \"song_data/*/*/*/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read song data file\n",
    "df = spark.read.json(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- artist_latitude: double (nullable = true)\n",
      " |-- artist_location: string (nullable = true)\n",
      " |-- artist_longitude: double (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- num_songs: long (nullable = true)\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print df schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(artist_id='ARDR4AC1187FB371A1', artist_latitude=None, artist_location='', artist_longitude=None, artist_name='Montserrat Caballé;Placido Domingo;Vicente Sardinero;Judith Blegen;Sherrill Milnes;Georg Solti', duration=511.16363, num_songs=1, song_id='SOBAYLL12A8C138AF9', title='Sono andati? Fingevo di dormire', year=0),\n",
       " Row(artist_id='AREBBGV1187FB523D2', artist_latitude=None, artist_location='Houston, TX', artist_longitude=None, artist_name=\"Mike Jones (Featuring CJ_ Mello & Lil' Bran)\", duration=173.66159, num_songs=1, song_id='SOOLYAZ12A6701F4A6', title='Laws Patrolling (Album Version)', year=0),\n",
       " Row(artist_id='ARMAC4T1187FB3FA4C', artist_latitude=40.82624, artist_location='Morris Plains, NJ', artist_longitude=-74.47995, artist_name='The Dillinger Escape Plan', duration=207.77751, num_songs=1, song_id='SOBBUGU12A8C13E95D', title='Setting Fire to Sleeping Giants', year=2004),\n",
       " Row(artist_id='ARPBNLO1187FB3D52F', artist_latitude=40.71455, artist_location='New York, NY', artist_longitude=-74.00712, artist_name='Tiny Tim', duration=43.36281, num_songs=1, song_id='SOAOIBZ12AB01815BE', title='I Hold Your Hand In Mine [Live At Royal Albert Hall]', year=2000),\n",
       " Row(artist_id='ARDNS031187B9924F0', artist_latitude=32.67828, artist_location='Georgia', artist_longitude=-83.22295, artist_name='Tim Wilson', duration=186.48771, num_songs=1, song_id='SONYPOM12A8C13B2D7', title='I Think My Wife Is Running Around On Me (Taco Hell)', year=2005)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view firts data in df\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+---------------+------------------+-----------+------------------+---------+------------------+--------------------+-----------------+\n",
      "|summary|         artist_id|   artist_latitude|artist_location|  artist_longitude|artist_name|          duration|num_songs|           song_id|               title|             year|\n",
      "+-------+------------------+------------------+---------------+------------------+-----------+------------------+---------+------------------+--------------------+-----------------+\n",
      "|  count|                71|                31|             71|                31|         71|                71|       71|                71|                  71|               71|\n",
      "|   mean|              null| 36.55297161290323|           null|-73.25123258064517|       null|239.72967605633804|      1.0|              null|                null|785.9577464788732|\n",
      "| stddev|              null|12.431023413063544|           null| 36.05807592882607|       null|106.56277912134071|      0.0|              null|                null|980.9571191533839|\n",
      "|    min|AR051KA1187B98B2FF|           -13.442|               |        -122.42005|    40 Grit|          29.54404|        1|SOAOIBZ12AB01815BE|A Higher Place (A...|                0|\n",
      "|    max|ARYKCQI1187FB3B18F|          56.27609| Zagreb Croatia|           15.9676|  lextrical|         599.24853|        1|SOZVMJI12AB01808AF|   ¿Dónde va Chichi?|             2008|\n",
      "+-------+------------------+------------------+---------------+------------------+-----------+------------------+---------+------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# describe dataset\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Define **songs** table\n",
    " SQL Query to define songs table\n",
    "```sql\n",
    "song_table_insert = (\"\"\"\n",
    "INSERT INTO dim_song(song_id, title, artist_id, year, duration)\n",
    "SELECT DISTINCT song_id as song_id,\n",
    "                title as title,\n",
    "                artist_id as artist_id,\n",
    "                year as year,\n",
    "                duration as duration\n",
    "FROM staging_songs\n",
    "WHERE song_id IS NOT NULL;\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Songs table using Spark SQL API\n",
    "\n",
    "df.createOrReplaceTempView(\"staging_songs\")\n",
    "songs_table = spark.sql(\"\"\"\n",
    "SELECT DISTINCT song_id as song_id,\n",
    "                title as title,\n",
    "                artist_id as artist_id,\n",
    "                year as year,\n",
    "                duration as duration\n",
    "FROM staging_songs\n",
    "WHERE song_id IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+------------------+-----------------+------------------+\n",
      "|summary|           song_id|               title|         artist_id|             year|          duration|\n",
      "+-------+------------------+--------------------+------------------+-----------------+------------------+\n",
      "|  count|                71|                  71|                71|               71|                71|\n",
      "|   mean|              null|                null|              null|785.9577464788732|239.72967605633815|\n",
      "| stddev|              null|                null|              null| 980.957119153384|106.56277912134071|\n",
      "|    min|SOAOIBZ12AB01815BE|A Higher Place (A...|AR051KA1187B98B2FF|                0|          29.54404|\n",
      "|    max|SOZVMJI12AB01808AF|   ¿Dónde va Chichi?|ARYKCQI1187FB3B18F|             2008|         599.24853|\n",
      "+-------+------------------+--------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.printSchema()\n",
    "songs_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create songs table using Spark Dataframe API\n",
    "\n",
    "songs_table = df.select([\"song_id\", \"title\", \"artist_id\", \"year\", \"duration\"]) \\\n",
    "                .dropna(how = \"any\", subset = [\"song_id\", \"artist_id\"]) \\\n",
    "                .where(df[\"song_id\"].isNotNull()) \\\n",
    "                .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      "\n",
      "+-------+------------------+--------------------+------------------+-----------------+------------------+\n",
      "|summary|           song_id|               title|         artist_id|             year|          duration|\n",
      "+-------+------------------+--------------------+------------------+-----------------+------------------+\n",
      "|  count|                71|                  71|                71|               71|                71|\n",
      "|   mean|              null|                null|              null|785.9577464788732|239.72967605633815|\n",
      "| stddev|              null|                null|              null| 980.957119153384|106.56277912134071|\n",
      "|    min|SOAOIBZ12AB01815BE|A Higher Place (A...|AR051KA1187B98B2FF|                0|          29.54404|\n",
      "|    max|SOZVMJI12AB01808AF|   ¿Dónde va Chichi?|ARYKCQI1187FB3B18F|             2008|         599.24853|\n",
      "+-------+------------------+--------------------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songs_table.printSchema()\n",
    "songs_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.mode(\"overwrite\") \\\n",
    "                .partitionBy(\"year\", \"artist_id\") \\\n",
    "                .parquet((output_data + \"songs_table.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Define artists table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Query to define artist table\n",
    "```sql\n",
    "artist_table_insert = (\"\"\"\n",
    "INSERT INTO dim_artist(artist_id, name, location, latitude, longitude)\n",
    "SELECT DISTINCT artist_id as artist_id,\n",
    "                artist_name as name,\n",
    "                artist_location as location,\n",
    "                artist_latitude as latitude,\n",
    "                artist_longitude as longitude\n",
    "FROM staging_songs\n",
    "where artist_id IS NOT NULL;\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artist table using Spark SQL API\n",
    "\n",
    "df.createOrReplaceTempView(\"staging_songs\")\n",
    "artists_table = spark.sql(\"\"\"\n",
    "SELECT DISTINCT artist_id as artist_id,\n",
    "                artist_name as name,\n",
    "                artist_location as location,\n",
    "                artist_latitude as latitude,\n",
    "                artist_longitude as longitude\n",
    "FROM staging_songs\n",
    "where artist_id IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+-------+------------------+---------+--------------+------------------+------------------+\n",
      "|summary|         artist_id|     name|      location|          latitude|         longitude|\n",
      "+-------+------------------+---------+--------------+------------------+------------------+\n",
      "|  count|                69|       69|            69|                31|                31|\n",
      "|   mean|              null|     null|          null| 36.55297161290323|-73.25123258064517|\n",
      "| stddev|              null|     null|          null|12.431023413063542| 36.05807592882608|\n",
      "|    min|AR051KA1187B98B2FF|  40 Grit|              |           -13.442|        -122.42005|\n",
      "|    max|ARYKCQI1187FB3B18F|lextrical|Zagreb Croatia|          56.27609|           15.9676|\n",
      "+-------+------------------+---------+--------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.printSchema()\n",
    "artists_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artist table using Spark Dataframe API\n",
    "\n",
    "artists_table = df.selectExpr(\"artist_id\", \"artist_name as name\", \"artist_location as location\", \"artist_latitude as latitude\", \"artist_longitude as longitude\") \\\n",
    "                .dropna(how = \"any\", subset = [\"artist_id\"]) \\\n",
    "                .where(df[\"artist_id\"].isNotNull()) \\\n",
    "                .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "+-------+------------------+---------+--------------+------------------+------------------+\n",
      "|summary|         artist_id|     name|      location|          latitude|         longitude|\n",
      "+-------+------------------+---------+--------------+------------------+------------------+\n",
      "|  count|                69|       69|            69|                31|                31|\n",
      "|   mean|              null|     null|          null| 36.55297161290323|-73.25123258064515|\n",
      "| stddev|              null|     null|          null|12.431023413063542| 36.05807592882608|\n",
      "|    min|AR051KA1187B98B2FF|  40 Grit|              |           -13.442|        -122.42005|\n",
      "|    max|ARYKCQI1187FB3B18F|lextrical|Zagreb Croatia|          56.27609|           15.9676|\n",
      "+-------+------------------+---------+--------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "artists_table.printSchema()\n",
    "artists_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write artists table to parquet files\n",
    "artists_table.write.mode(\"overwrite\") \\\n",
    "                .parquet((output_data + \"artists_table.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Process Log Data : Define **songs_table** and **artists_table** from **songs_data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get filepath to log data file\n",
    "log_data = input_data + \"log_data/*/*/*.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read log data file\n",
    "df = spark.read.json(log_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----------+---------+------+------------------+--------+------------------+-----+--------------------+------+-------+--------------------+------------------+--------------------+-----------------+--------------------+--------------------+-----------------+\n",
      "|summary|            artist|      auth|firstName|gender|     itemInSession|lastName|            length|level|            location|method|   page|        registration|         sessionId|                song|           status|                  ts|           userAgent|           userId|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+------------------+-----+--------------------+------+-------+--------------------+------------------+--------------------+-----------------+--------------------+--------------------+-----------------+\n",
      "|  count|              6820|      8056|     7770|  7770|              8056|    7770|              6820| 8056|                7770|  8056|   8056|                7770|              8056|                6820|             8056|                8056|                7770|             8056|\n",
      "|   mean|             266.5|      null|     null|  null| 21.19885799404171|    null|247.03222091348962| null|                null|  null|   null|1.540777905900890...| 598.1675769612712|  1388.3636363636363| 202.897591857001|1.542486261744982...|                null|54.46396396396396|\n",
      "| stddev|109.00229355385143|      null|     null|  null|23.440698529423248|    null|102.97592081740872| null|                null|  null|   null|2.6515717661161613E8|285.31309422187996|  2347.5150807919113|17.99425562964626|  7.00316630206128E8|                null|28.16850353343071|\n",
      "|    min|               !!!| Logged In|   Adelyn|     F|                 0|Arellano|          15.85587| free|Atlanta-Sandy Spr...|   GET|  About|   1.539908999796E12|                 3| I Will Not Reap ...|              200|       1541105830796|\"Mozilla/5.0 (Mac...|                 |\n",
      "|    max|   ÃÂtienne Daho|Logged Out|  Zachary|     M|               127|   Young|        2594.87302| paid|       Yuba City, CA|   PUT|Upgrade|   1.541098488796E12|              1114|ÃÂ Aqui Que Se ...|              404|       1543607664796|Mozilla/5.0 (comp...|               99|\n",
      "+-------+------------------+----------+---------+------+------------------+--------+------------------+-----+--------------------+------+-------+--------------------+------------------+--------------------+-----------------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+---------+------+------------------+--------+------------------+-----+--------------------+------+--------+--------------------+-----------------+--------------------+------+--------------------+--------------------+------------------+\n",
      "|summary|            artist|     auth|firstName|gender|     itemInSession|lastName|            length|level|            location|method|    page|        registration|        sessionId|                song|status|                  ts|           userAgent|            userId|\n",
      "+-------+------------------+---------+---------+------+------------------+--------+------------------+-----+--------------------+------+--------+--------------------+-----------------+--------------------+------+--------------------+--------------------+------------------+\n",
      "|  count|              6820|     6820|     6820|  6820|              6820|    6820|              6820| 6820|                6820|  6820|    6820|                6820|             6820|                6820|  6820|                6820|                6820|              6820|\n",
      "|   mean|             266.5|     null|     null|  null|22.761143695014663|    null|247.03222091348962| null|                null|  null|    null|1.540778618854797...|599.1818181818181|  1388.3636363636363| 200.0|1.542485482323272...|                null| 54.68123167155425|\n",
      "| stddev|109.00229355385143|     null|     null|  null|23.444636081691684|    null|102.97592081740872| null|                null|  null|    null|2.6274240255198932E8|284.9533328431846|  2347.5150807919113|   0.0| 7.003235871646643E8|                null|28.162734412152382|\n",
      "|    min|               !!!|Logged In|   Adelyn|     F|                 0|Arellano|          15.85587| free|Atlanta-Sandy Spr...|   PUT|NextSong|   1.539908999796E12|                3| I Will Not Reap ...|   200|       1541106106796|\"Mozilla/5.0 (Mac...|                10|\n",
      "|    max|   ÃÂtienne Daho|Logged In|  Zachary|     M|               126|   Young|        2594.87302| paid|       Yuba City, CA|   PUT|NextSong|   1.541098488796E12|             1114|ÃÂ Aqui Que Se ...|   200|       1543607664796|Mozilla/5.0 (comp...|                99|\n",
      "+-------+------------------+---------+---------+------+------------------+--------+------------------+-----+--------------------+------+--------+--------------------+-----------------+--------------------+------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter by actions for song plays\n",
    "df = df.filter(df.page == \"NextSong\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 define users_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Query to define users table\n",
    "```sql\n",
    "user_table_insert = (\"\"\"\n",
    "INSERT INTO dim_user(user_id, first_name, last_name, gender, level)\n",
    "SELECT DISTINCT userId as user_id,\n",
    "                firstName as first_name,\n",
    "                lastName as last_name,\n",
    "                gender as gender,\n",
    "                level as level\n",
    "FROM staging_events\n",
    "where userId IS NOT NULL;\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table using Spark SQL API\n",
    "\n",
    "df.createOrReplaceTempView(\"staging_events\")\n",
    "users_table = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT userId as user_id,\n",
    "                    firstName as first_name,\n",
    "                    lastName as last_name,\n",
    "                    gender as gender,\n",
    "                    level as level\n",
    "    FROM staging_events\n",
    "    where userId IS NOT NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----------+---------+------+-----+\n",
      "|summary|           user_id|first_name|last_name|gender|level|\n",
      "+-------+------------------+----------+---------+------+-----+\n",
      "|  count|               104|       104|      104|   104|  104|\n",
      "|   mean| 51.50961538461539|      null|     null|  null| null|\n",
      "| stddev|29.035296629035347|      null|     null|  null| null|\n",
      "|    min|                10|    Adelyn| Arellano|     F| free|\n",
      "|    max|                99|   Zachary|    Young|     M| paid|\n",
      "+-------+------------------+----------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table.printSchema()\n",
    "users_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns for users table using Spark Dataframe API\n",
    "\n",
    "users_table = df.selectExpr(\"userId as user_id\", \"firstName as first_name\", \"lastName as last_name\", \"gender as gender\", \"level as level\") \\\n",
    "                .dropna(how = \"any\", subset = [\"user_id\"]) \\\n",
    "                .where(df[\"userId\"].isNotNull()) \\\n",
    "                .dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      "\n",
      "+-------+------------------+----------+---------+------+-----+\n",
      "|summary|           user_id|first_name|last_name|gender|level|\n",
      "+-------+------------------+----------+---------+------+-----+\n",
      "|  count|               104|       104|      104|   104|  104|\n",
      "|   mean| 51.50961538461539|      null|     null|  null| null|\n",
      "| stddev|29.035296629035354|      null|     null|  null| null|\n",
      "|    min|                10|    Adelyn| Arellano|     F| free|\n",
      "|    max|                99|   Zachary|    Young|     M| paid|\n",
      "+-------+------------------+----------+---------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_table.printSchema()\n",
    "users_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write users table to parquet files\n",
    "users_table.write.mode(\"overwrite\") \\\n",
    "                .parquet((output_data + \"users_table.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 define timestamp_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Query to define users table\n",
    "```sql\n",
    "time_table_insert = (\"\"\"\n",
    "INSERT INTO dim_time(start_time, hour, day, week, month, year, weekday)\n",
    "SELECT distinct ts,\n",
    "                EXTRACT(hour from ts),\n",
    "                EXTRACT(day from ts),\n",
    "                EXTRACT(week from ts),\n",
    "                EXTRACT(month from ts),\n",
    "                EXTRACT(year from ts),\n",
    "                EXTRACT(weekday from ts)\n",
    "FROM staging_events\n",
    "WHERE ts IS NOT NULL;\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create timestamp column from original timestamp column ts\n",
    "get_timestamp = udf(lambda x:datetime.fromtimestamp(int(int(x)/1000)), TimestampType())\n",
    "\n",
    "df = df.withColumn(\"start_time\", get_timestamp(df.ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table using Spark SQL API\n",
    "df.createOrReplaceTempView(\"staging_events\")\n",
    "\n",
    "time_table =spark.sql(\"\"\"\n",
    "                        SELECT\n",
    "                            start_time,\n",
    "                            hour(start_time) as hour,\n",
    "                            dayofmonth(start_time) as day,\n",
    "                            weekofyear(start_time) as week,\n",
    "                            month(start_time) as month,\n",
    "                            year(start_time) as year,\n",
    "                            dayofweek(start_time) as weekday      \n",
    "                        FROM\n",
    "                            (\n",
    "                            SELECT DISTINCT start_time\n",
    "                            FROM staging_events as log\n",
    "                            WHERE log.ts IS NOT NULL\n",
    "                            )\n",
    "                        ORDER BY start_time\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----+------+-----------------+\n",
      "|summary|              hour|               day|              week|month|  year|          weekday|\n",
      "+-------+------------------+------------------+------------------+-----+------+-----------------+\n",
      "|  count|              6813|              6813|              6813| 6813|  6813|             6813|\n",
      "|   mean|13.476442095992955|17.253192426243945|46.380742697783646| 11.0|2018.0|4.181124321150741|\n",
      "| stddev| 5.892004556825478|  8.10917362423492|1.1835604099384778|  0.0|   0.0|1.727828792226424|\n",
      "|    min|                 0|                 1|                44|   11|  2018|                1|\n",
      "|    max|                23|                30|                48|   11|  2018|                7|\n",
      "+-------+------------------+------------------+------------------+-----+------+-----------------+\n",
      "\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-01 21:01:46|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:05:52|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:08:16|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:11:13|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:17:33|  21|  1|  44|   11|2018|      5|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.printSchema()\n",
    "time_table.describe().show()\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract columns to create time table using Spark DAtaframe API\n",
    "\n",
    "time_table = (df.withColumn(\"hour\", hour(df.start_time))\n",
    "                .withColumn(\"day\", dayofmonth(df.start_time))\n",
    "                .withColumn(\"week\", weekofyear(df.start_time))\n",
    "                .withColumn(\"month\", month(df.start_time))\n",
    "                .withColumn(\"year\", year(df.start_time))\n",
    "                .withColumn(\"weekday\", dayofweek(df.start_time))\n",
    "                .select([\"start_time\", \"hour\", \"day\", \"week\", \"month\", \"year\", \"weekday\"])\n",
    "                .dropDuplicates()\n",
    "                .orderBy(\"start_time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- start_time: timestamp (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- weekday: integer (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----+------+-----------------+\n",
      "|summary|              hour|               day|              week|month|  year|          weekday|\n",
      "+-------+------------------+------------------+------------------+-----+------+-----------------+\n",
      "|  count|              6813|              6813|              6813| 6813|  6813|             6813|\n",
      "|   mean|13.476442095992955|17.253192426243945|46.380742697783646| 11.0|2018.0|4.181124321150741|\n",
      "| stddev| 5.892004556825478|  8.10917362423492|1.1835604099384778|  0.0|   0.0|1.727828792226424|\n",
      "|    min|                 0|                 1|                44|   11|  2018|                1|\n",
      "|    max|                23|                30|                48|   11|  2018|                7|\n",
      "+-------+------------------+------------------+------------------+-----+------+-----------------+\n",
      "\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|         start_time|hour|day|week|month|year|weekday|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "|2018-11-01 21:01:46|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:05:52|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:08:16|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:11:13|  21|  1|  44|   11|2018|      5|\n",
      "|2018-11-01 21:17:33|  21|  1|  44|   11|2018|      5|\n",
      "+-------------------+----+---+----+-----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "time_table.printSchema()\n",
    "time_table.describe().show()\n",
    "time_table.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode(\"overwrite\") \\\n",
    "                .partitionBy(\"year\", \"month\") \\\n",
    "                .parquet((output_data + \"time_table.parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 define songplays_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL Query to define users table\n",
    "```sql\n",
    "songplay_table_insert = (\"\"\"\n",
    "INSERT INTO fact_songplay(start_time, user_id, level, song_id, artist_id, session_id, location, user_agent)\n",
    "SELECT DISTINCT to_timestamp(to_char(se.ts, '9999-99-99 99:99:99'),'YYYY-MM-DD HH24:MI:SS'),\n",
    "                se.userId as user_id,\n",
    "                se.level as level,\n",
    "                ss.song_id as song_id,\n",
    "                ss.artist_id as artist_id,\n",
    "                se.sessionId as session_id,\n",
    "                se.location as location,\n",
    "                se.userAgent as user_agent\n",
    "FROM staging_events se\n",
    "JOIN staging_songs ss ON se.song = ss.title AND se.artist = ss.artist_name;\n",
    "\"\"\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in song data to use for songplays table\n",
    "song_df = spark.read.parquet(output_data + \"songs_table.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "song_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define temp table\n",
    "\n",
    "df.createOrReplaceTempView(\"staging_events\")\n",
    "song_df.createOrReplaceTempView(\"staging_songs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- song_id: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- duration: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- artist_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM staging_songs\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- artist: string (nullable = true)\n",
      " |-- auth: string (nullable = true)\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- itemInSession: long (nullable = true)\n",
      " |-- lastName: string (nullable = true)\n",
      " |-- length: double (nullable = true)\n",
      " |-- level: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- method: string (nullable = true)\n",
      " |-- page: string (nullable = true)\n",
      " |-- registration: double (nullable = true)\n",
      " |-- sessionId: long (nullable = true)\n",
      " |-- song: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      " |-- ts: long (nullable = true)\n",
      " |-- userAgent: string (nullable = true)\n",
      " |-- userId: string (nullable = true)\n",
      " |-- start_time: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "        SELECT *\n",
    "        FROM staging_events\n",
    "\"\"\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define songplays using Spark Dataframe API\n",
    "songplays_table = (df.join(song_df, df.song == song_df.title, how=\"left\")\n",
    "                     .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "                     .withColumn(\"year\", year(df.start_time))\n",
    "                     .withColumn(\"month\", month(df.start_time))\n",
    "                     .selectExpr(\"songplay_id\", \"start_time\", \"year\", \"month\", \"userId as user_id\", \"level\", \n",
    "                                 \"song_id\", \"artist_id\", \"sessionId as session_id\", \"location\", \n",
    "                                 \"userAgent as user_agent\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+--------------------+-----+------------------+-----+------------------+------------------+-----------------+--------------------+--------------------+\n",
      "|summary|         songplay_id|                year|month|           user_id|level|           song_id|         artist_id|       session_id|            location|          user_agent|\n",
      "+-------+--------------------+--------------------+-----+------------------+-----+------------------+------------------+-----------------+--------------------+--------------------+\n",
      "|  count|                6820|                6820| 6820|              6820| 6820|                 4|                 4|             6820|                6820|                6820|\n",
      "|   mean|1.833988878894428...|              2018.0| 11.0| 54.68123167155425| null|              null|              null|599.1818181818181|                null|                null|\n",
      "| stddev|1.590451413093264...|1.714303188838417...|  0.0|28.162734412152382| null|              null|              null|284.9533328431846|                null|                null|\n",
      "|    min|                   0|                2018|   11|                10| free|SOGDBUF12A8C140FAA|AR558FS1187FB45658|                3|Atlanta-Sandy Spr...|\"Mozilla/5.0 (Mac...|\n",
      "|    max|         60129542195|                2018|   11|                99| paid|SOZCTXZ12AB0182364|AR5KOSW1187FB35FF4|             1114|       Yuba City, CA|Mozilla/5.0 (comp...|\n",
      "+-------+--------------------+--------------------+-----+------------------+-----+------------------+------------------+-----------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "songplays_table.head()\n",
    "songplays_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write songplays table to parquet files partitioned by year and month\n",
    "\n",
    "songplays_table.write.mode(\"overwrite\") \\\n",
    "                    .partitionBy(\"year\", \"month\") \\\n",
    "                    .parquet(output_data + \"songplays_table.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
